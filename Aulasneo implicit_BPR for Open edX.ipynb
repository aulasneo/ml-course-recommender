{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit Bayesian Personalized Ranking for Open edX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acquire the implicit BPR algorithm to use in AWS Sagemaker, subscribe to it from the marketplace in https://aws.amazon.com/marketplace/pp/prodview-xgpovurjdgtga?ref_=beagle&applicationId=AWSMPContessa\n",
    "\n",
    "You will have to create an IAM role with access to SageMager and S3 resources.\n",
    "\n",
    "Once you have this done, paste the ARN of the role and the algorithm here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = \"Paste the role ARN here\"\n",
    "algo_arn = \"Paste the algorithm ARN here\"\n",
    "bucket = \"Paste here the name of the bucket to store the data\"\n",
    "test_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Warning:</b> don't run this notebook on a production Open edX server. You can run the following command in a production server to upload the data, but the notebook should be hosted locally or in a separated instance.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the enrollment data into a CSV file.\n",
    "Remember to set the correct user and password.\n",
    "\n",
    "Options to do this successfully:\n",
    "- Work with a local copy of the database\n",
    "- Add a `-h <hostname>` option to the mysql command to connect to a remote db (check that you have access to it)\n",
    "- Run the following command command in the Open edX instance and copy the output file to the jupyter notebook directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!export MYSQL_PWD=<mysql password>; mysql -u root -b edxapp -e \"select user_id, course_id as item_id from student_courseenrollment;\" | tr '\\t' ',' > student_courseenrollment.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head student_courseenrollment.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare course enrollments training data <a id=\"prepare-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('student_courseenrollment.csv')\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to filter users with more than N enrollments.\n",
    "\n",
    "First we count enrollments per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollments_counter = df.pivot_table(index=['user_id'], aggfunc={'user_id': 'size'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "user_index = df.set_index('user_id').index\n",
    "filtered_users = enrollments_counter[enrollments_counter.user_id > N].index\n",
    "filtered_df = df[user_index.isin(filtered_users)]\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the test dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now filter out a random number of records from the training dataset, and use them to create a test dataset. Later on, we will compare this test dataset against the ranking inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = filtered_df.sample(test_size)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we remove the test records from the dataframe. This will be our final training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = filtered_df.drop(test_df.index)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data file <a id=\"create-training-data-file\"></a>\n",
    "\n",
    "Create a csv file from the dataframe above. Do not include the index, but include headers `user_id`, and `item_id` where each row is an enrollment. Show the head of the file and number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'aulasneo-training'\n",
    "train_data_file = '{}/student_courseenrollment-train.csv'.format(train_data_dir)\n",
    "\n",
    "!mkdir -p {train_data_dir}\n",
    "train_df[[\"user_id\", \"item_id\"]].to_csv(train_data_file, index=False)\n",
    "\n",
    "!head {train_data_file}\n",
    "!wc -l {train_data_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload training data to s3 <a id=\"upload-training-data\"></a>\n",
    "\n",
    "Choose a bucket in the correct region, optionally customize the prefix, and upload the csv created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "prefix = \"implicit-bpr-test\"\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "training_data = sagemaker_session.upload_data(train_data_file, bucket, \"{}/training\".format(prefix))\n",
    "\"uploaded training data file to {}\".format(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create a model <a id=\"create-model\"></a>\n",
    "\n",
    "### Run a SageMaker training job <a id=\"run-training-job\"></a>\n",
    "\n",
    "Provide a proper role and the algorithm arn from your subscription in the proper region. This code will start a training job, wait for it to be done, and report its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "job_name_prefix = 'implicit-bpr-test'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "estimator = sagemaker.AlgorithmEstimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    algorithm_arn = algo_arn,\n",
    "    role=role_arn,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    input_mode='File',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, job_name_prefix),\n",
    "    base_job_name=job_name_prefix\n",
    ")\n",
    "\n",
    "inputs = {\"training\": training_data}\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker model <a id=\"create-sagemaker-model\"></a>\n",
    "\n",
    "This will set up a model_package and model within SageMaker from the artifacts created during training. This will be used later for recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_name = estimator.latest_training_job.name\n",
    "sagemaker_session.create_model_package_from_algorithm(model_name, 'test', algo_arn, estimator.model_data)\n",
    "sagemaker_session.wait_for_model_package(model_name, poll=5)\n",
    "sagemaker_session.create_model(model_name, role_arn, [{'ModelPackageName': model_name}], enable_network_isolation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Get recommendations (Inference) <a id=\"get-recommendations\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batch transform input file <a id=\"create-batch-input\"></a>\n",
    "\n",
    "Each row is a json object containing two keys:\n",
    "\n",
    "* `user_id`: the id of user\n",
    "* `top_n`: the number of top scoring recommendations to return\n",
    "\n",
    "The head of the batch input file is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "ranking_size = 10\n",
    "\n",
    "batch_input_dir = 'batch_input'\n",
    "batch_input_file = batch_input_dir + '/recommendation.requests'\n",
    "\n",
    "!mkdir -p {batch_input_dir}\n",
    "\n",
    "unique_users_in_test_df = test_df[\"user_id\"].drop_duplicates()\n",
    "\n",
    "with open(batch_input_file, 'w') as outfile:\n",
    "    for user_id in unique_users_in_test_df:\n",
    "        json.dump({\"user_id\": str(user_id), \"top_n\": str(ranking_size)}, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "   \n",
    "!head {batch_input_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the batch transform input file to s3 <a id=\"upload-batch-input\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = sagemaker_session.upload_data(batch_input_dir, bucket, \"{}/batch_input\".format(prefix))\n",
    "\"uploaded training data file to {}\".format(batch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Batch Transform Job <a id=\"run-transform\"></a>\n",
    "\n",
    "This code will start a batch transform job, wait for it to be done, and report its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "boto3_session = boto3.Session()\n",
    "sage = boto3_session.client(service_name='sagemaker')\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "batch_job_name = \"implicit-bpr-test\" + timestamp\n",
    "batch_output = 's3://{}/{}/output'.format(bucket, batch_job_name)\n",
    "request = \\\n",
    "{\n",
    "  \"TransformJobName\": batch_job_name,\n",
    "  \"ModelName\": model_name,\n",
    "  \"BatchStrategy\": \"SingleRecord\",\n",
    "  \"TransformInput\": {\n",
    "    \"DataSource\": {\n",
    "      \"S3DataSource\": {\n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"S3Uri\": batch_input\n",
    "      }\n",
    "    },\n",
    "    \"ContentType\": \"application/json\",\n",
    "    \"CompressionType\": \"None\",\n",
    "    \"SplitType\": \"Line\"\n",
    "  },\n",
    "  \"TransformOutput\": {\n",
    "    \"S3OutputPath\": batch_output,\n",
    "    \"Accept\": \"text/csv\",\n",
    "    \"AssembleWith\": \"Line\"\n",
    "  },\n",
    "  \"TransformResources\": {\n",
    "    \"InstanceType\": \"ml.c5.xlarge\",\n",
    "    \"InstanceCount\": 1\n",
    "  }\n",
    "}\n",
    "\n",
    "sage.create_transform_job(**request)\n",
    "\n",
    "print(\"Created Transform job with name: \", batch_job_name)\n",
    "\n",
    "while True :\n",
    "    job_info = sage.describe_transform_job(TransformJobName=batch_job_name)\n",
    "    status = job_info['TransformJobStatus']\n",
    "    if status == 'Completed':\n",
    "        print(\"Transform job ended with status: \" + status)\n",
    "        break\n",
    "    if status == 'Failed':\n",
    "        message = job_info['FailureReason']\n",
    "        print('Transform failed with the following error: {}'.format(message))\n",
    "        raise Exception('Transform job failed') \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the batch results <a id=\"download-batch-results\"></a>\n",
    "\n",
    "Download the results from S3 and show the head of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp {batch_output + '/recommendation.requests.out'} .\n",
    "\n",
    "!head recommendation.requests.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations with scores <a id=\"recommendations\"></a>\n",
    "\n",
    "Import the recommendations from the batch output file downloaded above. These are the top course recommendations for our example users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l recommendation.requests.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df = pd.read_csv('recommendation.requests.out', \n",
    "                                 header=None, \n",
    "                                 names=[\"user_id\", \"item_id\", \"score\"])\n",
    "\n",
    "recommendations_df[\"position\"] = list(range(1, ranking_size+1)) * int(len(recommendations_df)/ranking_size)\n",
    "\n",
    "recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now randomly take one of the users from out test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user = test_df.sample()[\"user_id\"].iloc[0]\n",
    "\n",
    "sample_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the courses that our selected user took in the past. Note that we have hidden one enrollment to the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df[train_df[\"user_id\"]==sample_user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the enrollment that our sample user has taken, and we have hidden to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df[\"user_id\"]==sample_user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this course would be positioned in the personal recommendation ranking for our sample user, if he hadn't taken it before. If it is in the ranking, it will be highlighted in the next table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_for_user = recommendations_df[recommendations_df[\"user_id\"]==sample_user]\n",
    "course = test_df[test_df[\"user_id\"]==sample_user]['item_id'].iloc[0]\n",
    "\n",
    "def highlight_item(x, item):\n",
    "    if x == item:\n",
    "        return 'background-color: yellow'\n",
    "    else:\n",
    "        return 'background-color: white'\n",
    "\n",
    "check_for_user.style.applymap(highlight_item, item=course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this for other users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user = test_df.sample()[\"user_id\"].iloc[0]\n",
    "check_for_user = recommendations_df[recommendations_df[\"user_id\"]==sample_user]\n",
    "course = test_df[test_df[\"user_id\"]==sample_user]['item_id'].iloc[0]\n",
    "print(\"User: {}, Course: {}\".format(sample_user, course))\n",
    "check_for_user.style.applymap(highlight_item, item=course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many, out of the actual enrollments that we have hidden to the algorithm, would have appear in the personal recommendation, and at which position in the ranking (1=top recommendation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_to_check = test_df.set_index([\"user_id\", \"item_id\"])\n",
    "\n",
    "check_df = recommendations_df.set_index([\"user_id\", \"item_id\"]).join(test_df_to_check, on=[\"user_id\", \"item_id\"], how=\"inner\")\n",
    "\n",
    "print(\"{} out of {} actual enrollments fell into the top {} personal ranking\".format(check_df['position'].count(), test_size, ranking_size))\n",
    "check_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram graph shows how many actual enrollments from the test data set appeard in each position in the user's personal ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = check_df.groupby(\"position\").count()\n",
    "zeros = pd.DataFrame(index=list(range(1,ranking_size + 1)), columns=[\"score\"], data=[0] * ranking_size)\n",
    "(bars + zeros).fillna(0).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells will create two csv files to log each run of this notebook. Then they are used to calculate the average times the user's choice falls into the ranking, and make a box plot to compare the positions in the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "\n",
    "with open('scores.csv', 'a+', newline='') as f:\n",
    "    csv_writer = writer(f)\n",
    "    csv_writer.writerow((bars + zeros).fillna(0)['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top10counts.csv', 'a+', newline='') as f:\n",
    "    csv_writer = writer(f)\n",
    "    csv_writer.writerow([check_df['position'].count()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10counts = pd.read_csv('top10counts.csv', header=None)\n",
    "scores = pd.read_csv('scores.csv', header=None, names = list(range(1,11)))\n",
    "\n",
    "print (\"{}% of the test enrollments fell into the top-{} ranking (average from of {} runs)\".format(top10counts.mean()[0]*100/test_size, ranking_size, len(scores)))\n",
    "\n",
    "ax=scores.boxplot(grid=False)\n",
    "ax.set_xlabel(\"Position in ranking\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Optional Clean up <a id=\"cleanup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    sagemaker_session.delete_model(model_name)\n",
    "    \n",
    "# optionally uncomment and run the code to clean everything up  \n",
    "\n",
    "#cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}